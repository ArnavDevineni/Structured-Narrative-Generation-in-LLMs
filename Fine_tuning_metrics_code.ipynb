{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHlAbqzDFDq"
      },
      "source": [
        "# Enhancing Structured Narrative Generation in Language Models: A Fine-Tuning Approach Utilizing Classic Short Stories\n",
        "\n",
        "## Abstract\n",
        "Storytelling is a fundamental human activity instrumental in communication and culture. Recent advancements in large language models (LLMs) have opened new possibilities in automated story generation. This project explores the fine-tuning of LLMs for dynamic and personalized story generation, capable of integrating user preferences into a coherent narrative structure inspired by classic short stories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Z-bzrKYPIO"
      },
      "source": [
        "## Introduction\n",
        "Incorporating the intricacies of human storytelling into machine learning models presents a complex challenge—a challenge that, if addressed, can transform how we interact with and consume stories. By tailoring narratives to individual user preferences, we aim to create a new dimension of engagement. Leveraging LLama 2 as the base model, this project aims to fine-tune this model by using differentiating techniques that augment their generative storytelling capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTrMu52YVCD"
      },
      "source": [
        "## Methodology\n",
        "We grounded our approach in parameter-efficient fine-tuning techniques, primarily focusing on quantized Learning Rate Annealing (qLoRA). A data-driven curriculum was developed to sequentially introduce the model to various facets of storytelling through a large dataset. User preferences are encoded using meta-data tags and injected into the model as conditional elements guiding the generation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjy9TVRWYdf-"
      },
      "source": [
        "## Experiments\n",
        "We conducted a series of experiments aimed at evaluating model performance of Llama 2 versus Mistral, and in comparing these two, we found that Mistral had an unsupportable compute power and thus we decided on using Llama 2 as our base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG8ffxfC9g4X"
      },
      "source": [
        "# Training Configuration\n",
        "\n",
        "Below is the training configuration for our [Llama 2](https://arxiv.org/abs/2307.09288) model.\n",
        "\n",
        "The training leverages [QLoRA](https://arxiv.org/abs/2305.14314), an efficient fine-tuning method that significantly reduces memory usage to enable training large models on resource-constrained environments. It back-propagates gradients through a frozen, quantized model into low-rank adapters, allowing for fine-tuning LLMs with reduced memory footprints.\n",
        "\n",
        "Key to this approach is the use of 4-bit precision loading of the base model, coupled with a highly optimized data type tailored for normally distributed weights. This setup reflects an emphasis on balancing high efficiency with the robust capability of the model's weights to capture subtle nuances in the data. Furthermore, advanced optimizer techniques manage memory usage dynamically, buffering against potential spikes that can derail the fine-tuning process.\n",
        "\n",
        "The training harnesses a streamlined batch processing and gradient accumulation strategy that enhances resource utilization without degrading the learning process. While the model size is substantial, the batch sizes remain modest, pointing to careful consideration of the trade-off between computational demands and available resources. Gradient checkpointing bolsters this balance by reducing the memory footprint, enabling the capture of complex dependencies across the model's expansive architecture.\n",
        "\n",
        "The fine-tuning process employs the [AdamW](https://arxiv.org/abs/1711.05101) optimizer, an adaptation of the traditional Adam optimizer which incorporates decoupled weight decay regularization. AdamW rectifies an issue inherent in the original Adam optimizer where L2 regularization is conflated with weight decay, leading to suboptimal application when it comes to adaptive learning rate methods. By decoupling the weight decay factor from the loss-based optimization steps, the AdamW optimizer provides a more principled approach to regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"siddrao11/test\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-storytelling-non-chat\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./drive/MyDrive/cs180/non-chat\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 1500\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 250\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Tt7v82YsWM"
      },
      "source": [
        "## Data\n",
        "This work collects a large dataset of 300,000 human-written stories paired with writing prompts from an online forum that enables hierarchical story generation, specifically found in the [Hierarchical Neural Story Generation](https://github.com/facebookresearch/fairseq/tree/main/examples/stories) github. Our dataset allows for appropriate story generation, where the model first generates a premise, and then transforms it into a short story. The processed dataset is available [here](https://huggingface.co/datasets/siddrao11/test)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp03yKQoaIN7"
      },
      "source": [
        "## Task\n",
        "The primary task was to generate structured, coherent, and personalized short stories using a fine-tuned model, challenging it to maintain narrative integrity while adapting to diverse user-defined elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "outputs": [],
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"formatted_text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# checkpoint_path = os.path.join(output_dir, 'checkpoint-5000')\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# # Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0tlYW6C6mH1"
      },
      "source": [
        "## Evaluation Protocol\n",
        "Performance is difficult to assess here because we are analyzing a generative model and we can’t necessarily directly compare to a solution, but what we can do is analyze a quantitative metric, such as the perplexity score, where the lower the perplexity score indicates a better response. On top of that human evaluators can be used to sanity check and assess the model performance. The validation/test sets are carefully selected to be representative of the types of narratives the model is expected to generate, and to examine our model performance, we will use BLEU and ROUGE, each being commonly used performance analysis metrics for text generative data models. These metrics compare size, similarity and structure of output texts to expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-TSs0CudW_O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frlSLPin4IJ4"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Load your trained model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "# Merge fine-tuned model\n",
        "model = PeftModel.from_pretrained(base_model, os.path.join(output_dir, 'checkpoint-1500'))\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "# prompt = \"\"\"Set in a bustling cyberpunk city (Setting), follow the story of Ava, a brilliant hacker, and Max, a rogue detective with a mysterious past (Characters). As they navigate the neon-lit streets, a powerful AI threatens to expose long-buried secrets, putting them on a collision course with both the law and criminal underworld (Plot Direction). Infuse the narrative with a sense of tension and intrigue (Emotional Tone). Incorporate elements of futuristic technology and unexpected alliances (Key Elements). Capture the essence of a gritty noir with a touch of unexpected humor (Style Preferences). Keep the story within 800 words and surprise me with an unexpected twist at the end (Constraints and Open-endedness).\"\"\"\n",
        "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1024)\n",
        "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "# print(result[0]['generated_text'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4yZVeeC3prD"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "[INST]\n",
        "The Whispering Oaks of Orléans: Imagine a small village in France where ancient oak trees have begun to whisper secrets to the locals. These trees, with leaves of shimmering silver, hold the key to an old mystery involving the village's history. The story revolves around the journey of a young historian and an artist as they unravel the whispers, leading them to hidden treasures, forgotten love stories, and the village's untold history. Explore themes of discovery, the connection between past and present, and the magical bond between humans and nature. What happens next?\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, return_full_text=False, tokenizer=tokenizer, max_length=1000, temperature=1.75, repetition_penalty=1.2)\n",
        "result = pipe(f\"<s>{prompt}\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkQCviG0Zta-"
      },
      "outputs": [],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "# del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQCThl0GVZT"
      },
      "source": [
        "### Training Evals (Loss, Perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7MOWZzS6pxI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize lists to store steps and losses from all checkpoints\n",
        "all_steps = []\n",
        "all_losses = []\n",
        "all_lr = []\n",
        "all_perplexities = []\n",
        "\n",
        "# List all checkpoint subdirectories in output_dir (assumes naming convention starts with \"checkpoint-\")\n",
        "checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-') and os.path.isdir(os.path.join(output_dir, d))]\n",
        "\n",
        "# Loop through each checkpoint directory\n",
        "for checkpoint_dir in sorted(checkpoint_dirs):\n",
        "    # Path to the trainer_state.json in the current checkpoint directory\n",
        "    trainer_state_path = os.path.join(output_dir, checkpoint_dir, 'trainer_state.json')\n",
        "\n",
        "    # Load the trainer_state.json file\n",
        "    with open(trainer_state_path, 'r') as f:\n",
        "        trainer_state = json.load(f)\n",
        "\n",
        "    # Extract the log_history field\n",
        "    log_history = trainer_state.get('log_history', [])\n",
        "\n",
        "    # Extract step and loss info and add to the lists\n",
        "    for entry in log_history:\n",
        "        if 'loss' in entry and 'step' in entry and 'learning_rate' in entry:\n",
        "            all_steps.append(entry['step'])\n",
        "            all_losses.append(entry['loss'])\n",
        "            all_lr.append(entry['learning_rate'])\n",
        "\n",
        "            perplexity = np.exp(entry['loss'])\n",
        "            all_perplexities.append(perplexity)\n",
        "\n",
        "# Check if we accumulated data\n",
        "if not all_steps:\n",
        "    raise ValueError(\"No loss information found. Please check if the checkpoints contain 'trainer_state.json' and 'log_history'.\")\n",
        "\n",
        "# Sorting the all_steps and all_losses based on step values\n",
        "sorted_indices = sorted(range(len(all_steps)), key=lambda k: all_steps[k])\n",
        "all_steps = [all_steps[i] for i in sorted_indices]\n",
        "all_losses = [all_losses[i] for i in sorted_indices]\n",
        "all_lr = [all_lr[i] for i in sorted_indices]\n",
        "all_perplexities = [all_perplexities[i] for i in sorted_indices]\n",
        "\n",
        "# Plot loss against steps from all checkpoints combined\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(all_steps, all_losses, label='Training Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Llama-2-7b-hf Training Loss Across All TimeSteps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot lr\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(all_steps, all_lr, label='Learning Rate')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Llama-2-7b-hf Learning Rate Across All TimeSteps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('lr.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot perplexity\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(all_steps, all_perplexities, label='Perplexity')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.title('Llama-2-7b-hf Perplexity Across All TimeSteps')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('perplexity.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3WgKgWQqAdq"
      },
      "source": [
        "## Metrics\n",
        "### Flesch-Kincaid Readability Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62b9IrJ4qFdu"
      },
      "outputs": [],
      "source": [
        "from textstat import flesch_kincaid_grade\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "\n",
        "outputs = []\n",
        "\n",
        "def evaluate_readability_with_pipeline(pipeline_model, test_dataset):\n",
        "\n",
        "    scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "\n",
        "            instruction, target = match.groups()\n",
        "            pipe = pipeline(task=\"text-generation\", model=pipeline_model, return_full_text=False, tokenizer=tokenizer, temperature=0.5, repetition_penalty=1.2, device=0, max_length=len(instruction)+512)\n",
        "            generated_text = pipe(instruction)[0]['generated_text']\n",
        "            outputs.append(generated_text)\n",
        "            # print(generated_text)\n",
        "            score = flesch_kincaid_grade(generated_text)\n",
        "            scores.append(score)\n",
        "            # print(score)\n",
        "    return scores\n",
        "\n",
        "def evaluate_readability_baseline(test_dataset):\n",
        "    scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            score = flesch_kincaid_grade(target)\n",
        "            scores.append(score)\n",
        "            # print(score)\n",
        "    return scores\n",
        "\n",
        "def evaluate_readability_outputs(outputs):\n",
        "    scores = []\n",
        "    for output in outputs:\n",
        "        score = flesch_kincaid_grade(output)\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_dataset(dataset_name, split=\"test\").select(range(100))\n",
        "\n",
        "# Regular expression to identify instructions and target text\n",
        "instruction_pattern = re.compile(r'<s>\\[INST\\] (.+?) \\[/INST\\](.+)<\\/s>')\n",
        "\n",
        "readability_baseline = evaluate_readability_baseline(test_dataset)\n",
        "print(\"Baseline Readability:\", readability_baseline)\n",
        "\n",
        "# readability_base_model = evaluate_readability_with_pipeline(base_model, test_dataset)\n",
        "readability_model = evaluate_readability_with_pipeline(model, test_dataset)\n",
        "# readability_model = evaluate_readability_outputs(outputs)\n",
        "print(\"Base Model Readability:\", readability_baseline)\n",
        "print(\"Model Readability:\", readability_model)\n",
        "\n",
        "print(\"out\", outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "522eNN6Ab0de"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# plt.hist(readability_baseline, bins=20, color='skyblue', edgecolor='black')\n",
        "# plt.title('Baseline Model Flesch-Kincaid Readability Score Distribution')\n",
        "# plt.xlabel('Readability Score')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.savefig('readability_distribution_baseline.png')\n",
        "# plt.show()\n",
        "\n",
        "# plt.hist(readability_base_model, bins=20, color='skyblue', edgecolor='black')\n",
        "# plt.title('Base Model Flesch-Kincaid Readability Score Distribution')\n",
        "# plt.xlabel('Readability Score')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()\n",
        "\n",
        "plt.hist(readability_model, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Llama-2-7b-hf Flesch-Kincaid Readability Score Distribution')\n",
        "plt.xlabel('Readability Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig('readability_distribution_Llama-2-7b-hf.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLfanvrSr5lK"
      },
      "source": [
        "### Lexical Diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcJZ_5Arr5CY"
      },
      "outputs": [],
      "source": [
        "def lexical_diversity(text):\n",
        "    words = text.split()  # Tokenize the text into words\n",
        "    unique_words = set(words)  # Find unique words\n",
        "    return len(unique_words) / len(words)  # Calculate lexical diversity\n",
        "\n",
        "\n",
        "def evaluate_lexical_diversity_with_pipeline(pipeline_model, test_dataset):\n",
        "\n",
        "    scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            pipe = pipeline(task=\"text-generation\", model=pipeline_model, return_full_text=False, tokenizer=tokenizer, temperature=0.5, repetition_penalty=1.5, device=0, max_length=len(instruction)+256)\n",
        "            generated_text = pipe(instruction)[0]['generated_text']\n",
        "            score = lexical_diversity(generated_text)\n",
        "            scores.append(score)\n",
        "    return scores\n",
        "\n",
        "def evaluate_lexical_diversity_baseline(test_dataset):\n",
        "    scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            score = lexical_diversity(target)\n",
        "            scores.append(score)\n",
        "    return scores\n",
        "\n",
        "def evaluate_lexical_diversity_on_outputs(outputs):\n",
        "    scores = []\n",
        "    for output in outputs:\n",
        "        score = lexical_diversity(output)\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "baseline_diversity_scores = evaluate_lexical_diversity_baseline(test_dataset)\n",
        "print(\"Baseline Lexical Diversity:\", baseline_diversity_scores)\n",
        "\n",
        "# diversity_scores = evaluate_lexical_diversity_with_pipeline(model, test_dataset)\n",
        "# print(\"Fine-tuned Lexical Diversity:\", diversity_scores)\n",
        "\n",
        "diversity_scores = evaluate_lexical_diversity_on_outputs(outputs)\n",
        "print(\"Fine-tuned Lexical Diversity:\", diversity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khc41vyfw0Kh"
      },
      "outputs": [],
      "source": [
        "# plt.hist(baseline_diversity_scores, bins=20, color='green', edgecolor='black')\n",
        "# plt.title('Baseline Lexical Diversity Score Distribution')\n",
        "# plt.xlabel('Lexical Diversity Score')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.savefig('lexical_diversity_baseline.png')\n",
        "# plt.show()\n",
        "\n",
        "plt.hist(diversity_scores, bins=20, color='green', edgecolor='black')\n",
        "plt.title('Llama-2-7b-hf Lexical Diversity Score Distribution')\n",
        "plt.xlabel('Lexical Diversity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig('lexical_diversity_Llama-2-7b-hf.png')  # Save the plot as an image\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh--ttmWstOT"
      },
      "source": [
        "### BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBp3c34AsvUW"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8A3CYq8s2Yq"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "def evaluate_bert_score_with_pipeline(pipeline_model, test_dataset):\n",
        "    P_scores, R_scores, F1_scores = [], [], []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            pipe = pipeline(task=\"text-generation\", model=pipeline_model, return_full_text=False, tokenizer=tokenizer, temperature=0.5, repetition_penalty=1.5, device=0, max_length=len(instruction)+256)\n",
        "            generated_text = pipe(instruction)[0]['generated_text']\n",
        "            P, R, F1 = score([generated_text], [target], lang='en')\n",
        "            P_scores.append(P.mean().item())\n",
        "            R_scores.append(R.mean().item())\n",
        "            F1_scores.append(F1.mean().item())\n",
        "    return P_scores, R_scores, F1_scores\n",
        "\n",
        "def evaluate_bert_score_baseline(test_dataset):\n",
        "    P_scores, R_scores, F1_scores = [], [], []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            P, R, F1 = score([target], [target], lang='en')\n",
        "            P_scores.append(P.mean().item())\n",
        "            R_scores.append(R.mean().item())\n",
        "            F1_scores.append(F1.mean().item())\n",
        "    return P_scores, R_scores, F1_scores\n",
        "\n",
        "def evaluate_bert_score_on_outputs(outputs):\n",
        "    P_scores, R_scores, F1_scores = [], [], []\n",
        "    for output in outputs:\n",
        "        P, R, F1 = score([output], [output], lang='en')\n",
        "        P_scores.append(P.mean().item())\n",
        "        R_scores.append(R.mean().item())\n",
        "        F1_scores.append(F1.mean().item())\n",
        "    return P_scores, R_scores, F1_scores\n",
        "\n",
        "# P_scores, R_scores, F1_scores = evaluate_bert_score_with_pipeline(model, test_dataset)\n",
        "P_scores_baseline, R_scores_baseline, F1_scores_baseline = evaluate_bert_score_baseline(test_dataset)\n",
        "P_scores, R_scores, F1_scores = evaluate_bert_score_on_outputs(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4LTtsxK3d2u"
      },
      "outputs": [],
      "source": [
        "# # Precision Plot\n",
        "# plt.plot(P_scores_baseline, label='Precision', color='blue')\n",
        "# plt.title('Baseline BERTScore Precision Over Dataset')\n",
        "# plt.xlabel('Sample Index')\n",
        "# plt.ylabel('Precision Score')\n",
        "# plt.legend()\n",
        "# plt.savefig('precision_baseline.png')  # Save the plot as an image\n",
        "# plt.show()\n",
        "\n",
        "# # Recall Plot\n",
        "# plt.plot(R_scores_baseline, label='Recall', color='red')\n",
        "# plt.title('Baseline BERTScore Recall Over Dataset')\n",
        "# plt.xlabel('Sample Index')\n",
        "# plt.ylabel('Recall Score')\n",
        "# plt.legend()\n",
        "# plt.savefig('recall_baseline.png')  # Save the plot as an image\n",
        "# plt.show()\n",
        "\n",
        "# # F1 Score Plot\n",
        "# plt.plot(F1_scores_baseline, label='F1 Score', color='purple')\n",
        "# plt.title('Baseline BERTScore F1 Score Over Dataset')\n",
        "# plt.xlabel('Sample Index')\n",
        "# plt.ylabel('F1 Score')\n",
        "# plt.legend()\n",
        "# plt.savefig('f1_score_baseline.png')  # Save the plot as an image\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Precision Plot\n",
        "plt.plot(P_scores, label='Precision', color='blue')\n",
        "plt.title('Llama-2-7b-hf BERTScore Precision Over Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Precision Score')\n",
        "plt.legend()\n",
        "plt.savefig('precision_Llama-2-7b-hf.png')  # Save the plot as an image\n",
        "plt.show()\n",
        "\n",
        "# Recall Plot\n",
        "plt.plot(R_scores, label='Recall', color='red')\n",
        "plt.title('Llama-2-7b-hf BERTScore Recall Over Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Recall Score')\n",
        "plt.legend()\n",
        "plt.savefig('recall_Llama-2-7b-hf.png')  # Save the plot as an image\n",
        "plt.show()\n",
        "\n",
        "# F1 Score Plot\n",
        "plt.plot(F1_scores, label='F1 Score', color='purple')\n",
        "plt.title('Llama-2-7b-hf BERTScore F1 Score Over Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.savefig('f1_score_Llama-2-7b-hf.png')  # Save the plot as an image\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLS980WS6604"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_bleu_with_pipeline(pipeline_model, test_dataset):\n",
        "    bleu_scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            generated_text = pipe(instruction)[0]['generated_text']\n",
        "            reference = target.split()\n",
        "            candidate = generated_text.split()\n",
        "            score = sentence_bleu([reference], candidate)\n",
        "            bleu_scores.append(score)\n",
        "    return bleu_scores\n",
        "\n",
        "def evaluate_bleu_on_outputs(outputs, targets):\n",
        "    bleu_scores = []\n",
        "    for output, target in zip(outputs, targets):\n",
        "        reference = target.split()\n",
        "        candidate = output.split()\n",
        "        score = sentence_bleu([reference], candidate)\n",
        "        bleu_scores.append(score)\n",
        "    return bleu_scores\n",
        "\n",
        "def evaluate_bleu_baseline(test_dataset):\n",
        "    bleu_scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            generated_text = instruction\n",
        "            reference = target.split()\n",
        "            candidate = generated_text.split()\n",
        "            score = sentence_bleu([reference], candidate)\n",
        "            bleu_scores.append(score)\n",
        "    return bleu_scores\n",
        "\n",
        "targets = [instruction_pattern.match(entry['formatted_text']).groups()[1] for entry in test_dataset]\n",
        "# bleu_scores = evaluate_bleu_with_pipeline(model, test_dataset)\n",
        "bleu_scores_baseline = evaluate_bleu_baseline(test_dataset)\n",
        "bleu_scores_outputs = evaluate_bleu_on_outputs(outputs, targets)\n",
        "\n",
        "print(f'BLEU Score on outputs: {bleu_scores_outputs}')\n",
        "print(f'BLEU Score on baseline: {bleu_scores_baseline}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRTlD_Cd8jtr"
      },
      "outputs": [],
      "source": [
        "# # BLEU Plots\n",
        "# plt.plot(bleu_scores_baseline, label='Baseline', color='blue')\n",
        "# plt.title('Baseline BLEU Score Over Dataset')\n",
        "# plt.xlabel('Sample Index')\n",
        "# plt.ylabel('BLEU Score')\n",
        "# plt.legend()\n",
        "# plt.savefig('bleu_score.png')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "plt.plot(bleu_scores_outputs, label='Llama-2-7b-hf', color='red')\n",
        "plt.title('Llama-2-7b-hf BLEU Score Over Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('BLEU Score')\n",
        "plt.legend()\n",
        "plt.savefig('bleu_score_Llama-2-7b-hf.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtoiM07s_zSq"
      },
      "source": [
        "Rouge Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaxuR7_W_zyM"
      },
      "outputs": [],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZmHpsX49AFR"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def evaluate_rouge_with_pipeline(pipeline_model, test_dataset):\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['column_name'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            generated_text = pipe(instruction)[0]['generated_text']\n",
        "            scores = rouge.get_scores(generated_text, target)\n",
        "            rouge_scores.append(scores)\n",
        "    return rouge_scores\n",
        "\n",
        "def evaluate_rouge_on_outputs(outputs, targets):\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = []\n",
        "    for output, target in zip(outputs, targets):\n",
        "        scores = rouge.get_scores(output, target)\n",
        "        rouge_scores.append(scores)\n",
        "    return rouge_scores\n",
        "\n",
        "def evaluate_rouge_baseline(test_dataset):\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = []\n",
        "    for entry in test_dataset:\n",
        "        match = instruction_pattern.match(entry['formatted_text'])\n",
        "        if match:\n",
        "            instruction, target = match.groups()\n",
        "            generated_text = instruction\n",
        "            scores = rouge.get_scores(generated_text, target)\n",
        "            rouge_scores.append(scores)\n",
        "    return rouge_scores\n",
        "\n",
        "# rouge_scores = evaluate_rouge_with_pipeline(model, test_dataset)\n",
        "rouge_scores_baseline = evaluate_rouge_baseline(test_dataset)\n",
        "rouge_scores_outputs = evaluate_rouge_on_outputs(outputs, targets)\n",
        "\n",
        "print(f'ROUGE Score on outputs: {rouge_scores_outputs}')\n",
        "print(f'ROUGE Score on baseline: {rouge_scores_baseline}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5qzEf0h98TG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# # Extract F1-scores for each ROUGE metric\n",
        "# rouge_1_f1_baseline = [score[0]['rouge-1']['f'] for score in rouge_scores_baseline]\n",
        "# rouge_2_f1_baseline = [score[0]['rouge-2']['f'] for score in rouge_scores_baseline]\n",
        "# rouge_l_f1_baseline = [score[0]['rouge-l']['f'] for score in rouge_scores_baseline]\n",
        "\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(rouge_1_f1_baseline, label='ROUGE-1 F1', color='blue')\n",
        "# plt.plot(rouge_2_f1_baseline, label='ROUGE-2 F1', color='red')\n",
        "# plt.plot(rouge_l_f1_baseline, label='ROUGE-L F1', color='green')\n",
        "\n",
        "# plt.title('Baseline ROUGE Scores Over Dataset')\n",
        "# plt.xlabel('Sample Index')\n",
        "# plt.ylabel('F1 Score')\n",
        "# plt.legend()\n",
        "# plt.savefig('rouge_score_baseline.png')\n",
        "# plt.show()\n",
        "\n",
        "rouge_1_f1_outputs = [score[0]['rouge-1']['f'] for score in rouge_scores_outputs]\n",
        "rouge_2_f1_outputs = [score[0]['rouge-2']['f'] for score in rouge_scores_outputs]\n",
        "rouge_l_f1_outputs = [score[0]['rouge-l']['f'] for score in rouge_scores_outputs]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rouge_1_f1_outputs, label='ROUGE-1 F1', color='blue')\n",
        "plt.plot(rouge_2_f1_outputs, label='ROUGE-2 F1', color='red')\n",
        "plt.plot(rouge_l_f1_outputs, label='ROUGE-L F1', color='green')\n",
        "\n",
        "plt.title('Llama-2-7b-hf ROUGE Scores Over Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.savefig('rouge_score_Llama-2-7b-hf.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
