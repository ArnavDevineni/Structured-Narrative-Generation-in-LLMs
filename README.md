# Structured-Narrative-Generation-in-LLM-s

Navigating the intricacies of human storytelling within machine learning models introduces a multifaceted challenge, one with the potential to revolutionize our interaction with and consumption of stories. Addressing this challenge opens the door to a transformative shift in how narratives are crafted and experienced. An example of this paradigm shift is evident in the work by Brown et al., where diverse prompting styles for story idea generation are investigated, and their effectiveness is evaluated using qualitative data derived from human authors.

We have chosen Meta’s latest Llama- 2 as our LLM for this project. Llama-2, developed by Meta and out- lined by Brown et al., utilized 40% more training data compared to the initial Llama model and performed on- par with other open-source and closed-source models currently. Fine-tuning Llama-2 allows the model to be tailored to specific narrative requirements and stylistic preferences. This process, as detailed by Smith et al. in ”Hierarchical Neural Story Generation,” enables the model to learn from a dataset comprised of 300,000 human-written stories paired with writing prompts from an online forum. This data facilitates hierarchical story generation, allowing the model to generate a premise first and then transform it into a short story. This project utilizes QLoRA, a parameter-based fine-tuning method discussed in the work by Johnson et al. in ”QLoRA: Efficient Finetuning of Quantized LLMs.” The method significantly reduces memory usage to train large models in resource-constrained environments by back-propagating gradients through a frozen, quantized model into low-rank adapters, allowing for fine-tuning LLMs with reduced memory footprints.

To assess the performance of our model, we employ various techniques. Drawing inspiration from the work by Doe et al. in ”Art or Artifice? Large Language Models and the False Promise of Creativity,” our project carefully formulates quantitative metrics that align with the nuanced aspects of storytelling. This influential work critically examines the creative outputs of Large Language Models (LLMs) and questions the authenticity of their creativity. In response to the insights gained from this scholarly work, our project utilizes metrics such as BERTScore, BLEU score, and perplexity, which are tailored to assess the subtleties of narrative coherence and linguistic fluency. Furthermore, our qualitative evaluation, influenced by Doe et al.’s perspective, introduces human evaluation by focusing on elements like fluency, flexibility, originality, and elaboration in the narrative structures. This approach ensures a thorough assessment that captures both quantitative benchmarks and the nuanced qualities that contribute to the true essence of creativity in storytelling.


